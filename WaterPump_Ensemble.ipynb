{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('WaterPumpData/features_train.csv')\n",
    "y_train = pd.read_csv('WaterPumpData/labels_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['label'] = y_train.status_group.apply(lambda x: 1 if x == 'functional' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "funder = x_train.funder.str.lower().value_counts()\n",
    "funder = funder[funder > 250]\n",
    "x_train['funder_mod'] = [i if i in funder else 'Unknown' for i in x_train.funder]\n",
    "\n",
    "installer = x_train.funder.str.lower().value_counts()\n",
    "installer = funder[funder > 250]\n",
    "installer\n",
    "x_train['installer_mod'] = [i if i in installer else 'Unknown' for i in x_train.installer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "\n",
    "validation_set = sample(list(x_train.id), len(x_train)//5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_train = x_train.loc[~x_train.id.isin(validation_set)]\n",
    "xx_validate = x_train.loc[x_train.id.isin(validation_set)]\n",
    "yy_train = y_train.loc[~y_train.id.isin(validation_set)].label\n",
    "yy_validate = y_train.loc[y_train.id.isin(validation_set)].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "def create_gradientbooster_model(feature_set, cat_var):\n",
    "    categorical_bool = [True if i in cat_var else False for i in feature_set]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    xx_train = x_train.copy()\n",
    "    xx_train[cat_var] = xx_train[cat_var].apply(le.fit_transform)\n",
    "    xxx_train = xx_train[feature_set].loc[~xx_train.id.isin(validation_set)]\n",
    "    xxx_validate = xx_train[feature_set].loc[xx_train.id.isin(validation_set)]\n",
    "    yy_train = y_train.loc[~y_train.id.isin(validation_set)].label\n",
    "    yy_validate = y_train.loc[y_train.id.isin(validation_set)].label\n",
    "    clf = HistGradientBoostingClassifier(categorical_features=categorical_bool)\n",
    "    clf.fit(xxx_train, yy_train)\n",
    "    \n",
    "    return clf, xxx_validate, pd.DataFrame(yy_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def create_randomforest_model(feature_set, cat_var):\n",
    "    categorical_bool = [True if i in cat_var else False for i in feature_set]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    xx_train = pd.get_dummies(x_train[feature_set], columns=cat_var)\n",
    "    xxx_train = xx_train.loc[~x_train.id.isin(validation_set)]\n",
    "    xxx_validate = xx_train.loc[x_train.id.isin(validation_set)]\n",
    "    yy_train = y_train.loc[~y_train.id.isin(validation_set)].label\n",
    "    yy_validate = y_train.loc[y_train.id.isin(validation_set)].label\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(xxx_train, yy_train)\n",
    "    \n",
    "    return clf, xxx_validate, pd.DataFrame(yy_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def pipeline(feature_set, numeric_set):\n",
    "    numeric_features = numeric_set\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_features =  [i for i in feature_set if i not in numeric_set]\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('num', numeric_transformer, numeric_features)\n",
    "        ])\n",
    "\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor)\n",
    "                         ])\n",
    "\n",
    "    clf.fit(xx_train)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_8 = ['amount_tsh', 'population', 'funder_mod', 'district_code', 'gps_height', 'region', 'public_meeting', 'scheme_management', 'permit', 'construction_year', 'extraction_type_group', 'management_group', 'payment_type', 'quality_group', 'quantity_group', 'source_type', 'waterpoint_type_group']\n",
    "categorical_variables = ['funder_mod', 'district_code', 'region', 'public_meeting', 'scheme_management', 'permit', 'extraction_type_group', 'management_group', 'payment_type', 'quality_group', 'quantity_group', 'source_type', 'waterpoint_type_group']\n",
    "\n",
    "clf = pipeline(feature_set_8, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_train = pd.DataFrame(clf.transform(xx_train).todense())\n",
    "xxx_validate = pd.DataFrame(clf.transform(xx_validate).todense())\n",
    "yyy_train = pd.DataFrame(yy_train)\n",
    "yyy_validate = pd.DataFrame(yy_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prateekagarwal2/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_boost_model = HistGradientBoostingClassifier()\n",
    "grad_boost_model.fit(xxx_train, yyy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prateekagarwal2/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model.fit(xxx_train, yyy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.5511 - accuracy: 0.7430\n",
      "Epoch 2/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.4588 - accuracy: 0.7911\n",
      "Epoch 3/25\n",
      "2970/2970 [==============================] - 7s 2ms/step - loss: 0.3972 - accuracy: 0.8200\n",
      "Epoch 4/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.3433 - accuracy: 0.8460\n",
      "Epoch 5/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.2987 - accuracy: 0.8654\n",
      "Epoch 6/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.2642 - accuracy: 0.8798\n",
      "Epoch 7/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.2356 - accuracy: 0.8915\n",
      "Epoch 8/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.2180 - accuracy: 0.8984\n",
      "Epoch 9/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.2041 - accuracy: 0.9053\n",
      "Epoch 10/25\n",
      "2970/2970 [==============================] - 6s 2ms/step - loss: 0.1956 - accuracy: 0.9101\n",
      "Epoch 11/25\n",
      "1487/2970 [==============>...............] - ETA: 2s - loss: 0.1822 - accuracy: 0.9112"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, concatenate, Input\n",
    "from tensorflow.keras.optimizers import Adagrad, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "ip = Input(shape=xxx_train.shape[1], name='ip')\n",
    "x1 = Dense(250, activation='relu')(ip)\n",
    "x2 = Dense(100, activation='relu')(x1)\n",
    "x3 = Dense(50, activation='relu')(x2)\n",
    "x4 = Dense(10, activation='relu')(x3)\n",
    "output = Dense(1, activation='sigmoid')(x4)\n",
    "\n",
    "nn_model = Model(inputs = ip, outputs=output)\n",
    "nn_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(xxx_train, yyy_train, epochs=25, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost_pred = grad_boost_model.predict(xxx_validate)\n",
    "random_forest_pred = random_forest_model.predict(xxx_validate)\n",
    "nn_pred = nn_model.predict(xxx_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_validate['gb_pred'] = grad_boost_pred\n",
    "yyy_validate['rf_pred'] = random_forest_pred\n",
    "yyy_validate['nn_pred_raw'] = nn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_validate['nn_pred'] = round(yyy_validate.nn_pred_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_validate[\"sum\"] = yyy_validate.gb_pred + yyy_validate.rf_pred + yyy_validate.nn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy_validate[\"consensus\"] = yyy_validate['sum'].apply(lambda x: 1 if x in [2, 3] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(yyy_validate.consensus == yyy_validate.label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(yyy_validate.rf_pred == yyy_validate.label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
